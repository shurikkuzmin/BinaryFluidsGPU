ldc, 1024x1024, block_size: 64
------------------------------
baseline: 943.50
only 3 propagation variables: 974.15
1+3 propagation variable: 972.39
1 propagation variable: 957.00

global memory propagation, 16x16 blocks, ldc
--------------------------------------------
1024^2: 695.00
4096^2: 699.85

1024^2:
with shared memory, no halo: 667.08
with shared memory, halo: 482.80
with interaction: 431.04
full sc model: 337.88
-> this is slower than the simple approach!

global memory propagation, sc
=============================
baseline is 366
without unnecessary syncthreads and block_size = 128: 376.43
without sychthreads, with intrinsic expf, bs=128: 435.94

full sc with 32x16 kernels: 356.84 (32x32 block in shared memory)
- CollideAndPropagate: l:52  s:4704  r:32  occ:(0.500000 tb:1 w:16 l:regs)
- had to force regcount to 32, which resulted in the above local mem spill
- with propagation done in a loop, the spill is even larger:
  CollideAndPropagate: l:72  s:4704  r:31  occ:(0.500000 tb:1 w:16 l:regs)
  and performance suffers more:
  200.56
- with loop unrolling:
  CollideAndPropagate: l:44  s:4704  r:32  occ:(0.500000 tb:1 w:16 l:regs)
  362.86
  (large improvement)
- shared memory load in a loop with unrolling:
  CollideAndPropagate: l:24  s:4704  r:32  occ:(0.500000 tb:1 w:16 l:regs)
  383.75
- 394.38 without unnecessary syncthreads()
- 422.30 with intrinsic expf
- 433.29 with intrinsic expf and potential func. stored in shared memory
- use of shared memory for propagation causes 40b of local memory use
  and a performance drop down to 334.35

global memory propagation, ldc, 1024^2
-----------------------------------------
16 block: 455.34
32 block: 660.82
64 block: 857.96
128 blck: 892.01
196 blck: 693.49
256 blck: 795.40
512 blck: 723.22

shared memory propagation, ldc, 1024^2
-------------------------------------------
16 blk: 458.35	occ:(0.250000 tb:8 w:8 l:device)
32 blk: 681.97	occ:(0.250000 tb:8 w:8 l:device)
64 blk: 973.28	occ:(0.500000 tb:8 w:16 l:device)
96 blk: 1000.78	occ:(0.468750 tb:5 w:15 l:regs)
128 bk: 1042.81 occ:(0.625000 tb:5 w:20 l:regs)
192 bk: 948.93	occ:(0.562500 tb:3 w:18 l:regs)
256 bk: 928.08	occ:(0.500000 tb:2 w:16 l:regs)

r:24

*********************************************************************************
 try 2: texture-based access (1024x1024, single phase SC model)
*********************************************************************************
performance for modified SC code: calculate rho field, but ignore interactions
this effectively sets the upper performance limit for what can be achieved with
the texture-mediated accesses:

bs = 64:  574
bs = 128: 588

texture-based rho field access, exp() interaction (intrinstic exp):

bs = 32:  302.80
bs = 64:  416.93
bs = 96:  403.33
bs = 128: 425.49 (675.72)
bs = 192: 392.70

*********************************************************************************
 texture-bases access, binary SC model
*********************************************************************************

texture-based rho/phi, binary SC model:
[baseline, bs=64, fast_math: 231.94]

bs = 64:  215.70 (270.40)
bs = 128: 207.09

*** texture, binary fe model

[baseline
bs = 64:  214.71 (250.82)
bs = 128: 210.57 (275.12)

]

bs = 64:  224.54 (289.87)
bs = 128: 220.10 (316.64)


